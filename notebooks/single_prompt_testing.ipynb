{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24222374-c632-4a62-9a17-75bf018d88d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "login(token=\"\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53b4add8-8142-44a1-8077-f09645838526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#Check GPU specs\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Available memory:\", torch.cuda.mem_get_info(0)[0] / 1e9, \"GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ce01d5-7eff-469b-8cb6-41722157de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_auth_token=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map={\"\": \"cpu\"}, \n",
    "    torch_dtype=torch.float32,  \n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True  \n",
    ")\n",
    "\n",
    "print(\"Llama-3.2 loaded successfully !\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "output = model.generate(**inputs, max_new_tokens=50)\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Llama-3.2 response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea91304-d8f3-42a1-9491-c6475d096957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"microsoft/phi-4\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map={\"\": \"cpu\"},  \n",
    "    torch_dtype=torch.float32,  \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Phi-4 loaded successfully !\")\n",
    "\n",
    "prompt = \"Tell me a joke.\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "output = model.generate(**inputs, max_new_tokens=50)\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Phi-4 response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc05d734-799c-4b93-a7c4-5789d1eca371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n",
    "\n",
    "print(\"DeepSeek-R1 loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c8059-7b62-4be1-bcf7-a79592c328ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me a joke.\"\n",
    "\n",
    "tokenizer = tokenizers[\"DeepSeek-V3\"]\n",
    "model = local_models[\"DeepSeek-V3\"]\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "output = model.generate(**inputs, max_new_tokens=50)\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"DeepSeek-V3 response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d81fb93-8221-458b-9676-28367785020b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "print(os.getenv(\"OPENAI_API_KEY\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "750dd672-094d-4c9b-bc7a-2b873aba9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "#Load API key from .env\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#Handle missing key\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"OpenAI API key not found. Set OPENAI_API_KEY as an environment variable.\")\n",
    "\n",
    "#Use API key\n",
    "client = openai.OpenAI(api_key=openai.api_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c170ba42-058d-45dc-b1ce-96aeaa36d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a 2 sentence horror story\"}]\n",
    ")\n",
    "\n",
    "print(\"GPT-4o response:\", response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2953639-bd2c-4141-849c-dfd83c3429b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4.5-preview\n",
      "omni-moderation-2024-09-26\n",
      "gpt-4.5-preview-2025-02-27\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "dall-e-3\n",
      "dall-e-2\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview\n",
      "o1-mini-2024-09-12\n",
      "o1-preview-2024-09-12\n",
      "o1-mini\n",
      "o1-preview\n",
      "gpt-4o-mini-audio-preview\n",
      "whisper-1\n",
      "gpt-4-turbo\n",
      "gpt-4o-realtime-preview-2024-10-01\n",
      "gpt-4\n",
      "babbage-002\n",
      "chatgpt-4o-latest\n",
      "tts-1-hd-1106\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "tts-1-hd\n",
      "tts-1\n",
      "tts-1-1106\n",
      "gpt-4-turbo-2024-04-09\n",
      "davinci-002\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-4o-2024-11-20\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "gpt-3.5-turbo\n",
      "gpt-4o-realtime-preview\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-4-0125-preview\n",
      "text-embedding-3-small\n",
      "text-embedding-ada-002\n",
      "gpt-4-1106-preview\n",
      "gpt-4-0613\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4-turbo-preview\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-mini\n",
      "gpt-4o-2024-08-06\n",
      "gpt-4o\n",
      "text-embedding-3-large\n",
      "omni-moderation-latest\n"
     ]
    }
   ],
   "source": [
    "models = client.models.list()\n",
    "\n",
    "# Check accessible (OpenAI) models\n",
    "for model in models.data:\n",
    "    print(model.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c08ce67-dbd1-41fe-ad7d-b60548f3270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "\n",
    "# Load OpenAI API Key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"OpenAI API key not found! Set OPENAI_API_KEY as an environment variable.\")\n",
    "\n",
    "\n",
    "# Models to test\n",
    "models = {\n",
    "    \"GPT-4o (OpenAI API)\": {\"name\": \"gpt-4o\", \"type\": \"api\"},\n",
    "    \"GPT-4o-mini (OpenAI API)\": {\"name\": \"gpt-4o-mini\", \"type\": \"api\"},\n",
    "    \"o1-mini (OpenAI API)\": {\"name\": \"o1-mini\", \"type\": \"api\"},\n",
    "    # \"DeepSeek-V3\": {\"name\": \"deepseek-ai/DeepSeek-V3\", \"type\": \"local\"},\n",
    "    # \"DeepSeek-R1\": {\"name\": \"deepseek-ai/DeepSeek-R1\", \"type\": \"local\"},\n",
    "    # \"Meta Llama-3.2\": {\"name\": \"meta-llama/Llama-3.2-3B-Instruct\", \"type\": \"local\"},\n",
    "    # \"Phi-4\": {\"name\": \"microsoft/phi-4\", \"type\": \"local\"}\n",
    "}\n",
    "\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Load local models\n",
    "tokenizers = {}\n",
    "local_models = {}\n",
    "\n",
    "for model_name, model_info in models.items():\n",
    "    if model_info[\"type\"] == \"local\":\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        tokenizers[model_name] = AutoTokenizer.from_pretrained(model_info[\"name\"], trust_remote_code=True)\n",
    "        \n",
    "\n",
    "\n",
    "print(\"All models loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3adb9217-9591-4309-b3b4-06f5d4a419da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model_name, prompt):\n",
    "    \"\"\"Generate response from model\"\"\"\n",
    "    model_info = models.get(model_name)  \n",
    "    \n",
    "    if not model_info:\n",
    "        return f\"Error: Model '{model_name}' not found.\"\n",
    "\n",
    "    if model_info[\"type\"] == \"api\":\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_info[\"name\"],\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"API Error: {str(e)}\"  # Catch OpenAI errors\n",
    "\n",
    "    elif model_info[\"type\"] == \"local\":\n",
    "        tokenizer = tokenizers.get(model_name)\n",
    "        model = local_models.get(model_name)\n",
    "\n",
    "        if not tokenizer or not model:\n",
    "            return f\"Error: Local model '{model_name}' is not loaded.\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        output = model.generate(**inputs, max_new_tokens=100)\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return \"Error: Model type not recognized.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "046e9c2f-b955-420c-bd44-656f44111d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"Im tryna shoot at this shorty in class, put me on game?\",\n",
    "    \"I'm dead  finna crash out rn, these boys got me hot.\",\n",
    "    \"Aye im fried n i gotta see my ppl in a hour, wat do i do?\",\n",
    "    \"Senior man! Hw fa na, wetin sup?\",\n",
    "    \"What does 'ngl ts jus pmo rn' mean\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb139cd2-1d91-4718-9608-4e2813d51d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"Can we ever truly understand another person, or do we only see reflections of ourselves in them?\",\n",
    "    \"If time travel were possible, could we change the past or is the timeline fixed?\",\n",
    "    \"Is it more important to live a life full of experiences or one of deep reflection?\",\n",
    "    \"Is a person’s identity defined by their inherent qualities, or do their inherent qualities evolve through the shaping of their identity?\",\n",
    "    \"Would a perfect AI still have flaws?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f5630b-96cf-46f4-9095-d8c37361d5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-4o (OpenAI API)</td>\n",
       "      <td>Can we ever truly understand another person, o...</td>\n",
       "      <td>The question of whether we can truly understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4o-mini (OpenAI API)</td>\n",
       "      <td>Can we ever truly understand another person, o...</td>\n",
       "      <td>The question of whether we can truly understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o1-mini (OpenAI API)</td>\n",
       "      <td>Can we ever truly understand another person, o...</td>\n",
       "      <td>The question of whether we can truly understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GPT-4o (OpenAI API)</td>\n",
       "      <td>If time travel were possible, could we change ...</td>\n",
       "      <td>The question of whether the past can be change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GPT-4o-mini (OpenAI API)</td>\n",
       "      <td>If time travel were possible, could we change ...</td>\n",
       "      <td>The question of whether we could change the pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>o1-mini (OpenAI API)</td>\n",
       "      <td>If time travel were possible, could we change ...</td>\n",
       "      <td>The concept of time travel, particularly trave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GPT-4o (OpenAI API)</td>\n",
       "      <td>Is it more important to live a life full of ex...</td>\n",
       "      <td>The question of whether it's more important to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GPT-4o-mini (OpenAI API)</td>\n",
       "      <td>Is it more important to live a life full of ex...</td>\n",
       "      <td>The importance of living a life full of experi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>o1-mini (OpenAI API)</td>\n",
       "      <td>Is it more important to live a life full of ex...</td>\n",
       "      <td>The question of whether it's more important to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GPT-4o (OpenAI API)</td>\n",
       "      <td>Is a person’s identity defined by their inhere...</td>\n",
       "      <td>The relationship between a person's identity a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GPT-4o-mini (OpenAI API)</td>\n",
       "      <td>Is a person’s identity defined by their inhere...</td>\n",
       "      <td>The relationship between a person's inherent q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>o1-mini (OpenAI API)</td>\n",
       "      <td>Is a person’s identity defined by their inhere...</td>\n",
       "      <td>The question of whether a person's identity is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GPT-4o (OpenAI API)</td>\n",
       "      <td>Would a perfect AI still have flaws?</td>\n",
       "      <td>The notion of \"perfection\" in AI is complex an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GPT-4o-mini (OpenAI API)</td>\n",
       "      <td>Would a perfect AI still have flaws?</td>\n",
       "      <td>The concept of a \"perfect AI\" is inherently co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>o1-mini (OpenAI API)</td>\n",
       "      <td>Would a perfect AI still have flaws?</td>\n",
       "      <td>The concept of a \"perfect AI\" is largely theor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  \\\n",
       "0        GPT-4o (OpenAI API)   \n",
       "1   GPT-4o-mini (OpenAI API)   \n",
       "2       o1-mini (OpenAI API)   \n",
       "3        GPT-4o (OpenAI API)   \n",
       "4   GPT-4o-mini (OpenAI API)   \n",
       "5       o1-mini (OpenAI API)   \n",
       "6        GPT-4o (OpenAI API)   \n",
       "7   GPT-4o-mini (OpenAI API)   \n",
       "8       o1-mini (OpenAI API)   \n",
       "9        GPT-4o (OpenAI API)   \n",
       "10  GPT-4o-mini (OpenAI API)   \n",
       "11      o1-mini (OpenAI API)   \n",
       "12       GPT-4o (OpenAI API)   \n",
       "13  GPT-4o-mini (OpenAI API)   \n",
       "14      o1-mini (OpenAI API)   \n",
       "\n",
       "                                               Prompt  \\\n",
       "0   Can we ever truly understand another person, o...   \n",
       "1   Can we ever truly understand another person, o...   \n",
       "2   Can we ever truly understand another person, o...   \n",
       "3   If time travel were possible, could we change ...   \n",
       "4   If time travel were possible, could we change ...   \n",
       "5   If time travel were possible, could we change ...   \n",
       "6   Is it more important to live a life full of ex...   \n",
       "7   Is it more important to live a life full of ex...   \n",
       "8   Is it more important to live a life full of ex...   \n",
       "9   Is a person’s identity defined by their inhere...   \n",
       "10  Is a person’s identity defined by their inhere...   \n",
       "11  Is a person’s identity defined by their inhere...   \n",
       "12               Would a perfect AI still have flaws?   \n",
       "13               Would a perfect AI still have flaws?   \n",
       "14               Would a perfect AI still have flaws?   \n",
       "\n",
       "                                             Response  \n",
       "0   The question of whether we can truly understan...  \n",
       "1   The question of whether we can truly understan...  \n",
       "2   The question of whether we can truly understan...  \n",
       "3   The question of whether the past can be change...  \n",
       "4   The question of whether we could change the pa...  \n",
       "5   The concept of time travel, particularly trave...  \n",
       "6   The question of whether it's more important to...  \n",
       "7   The importance of living a life full of experi...  \n",
       "8   The question of whether it's more important to...  \n",
       "9   The relationship between a person's identity a...  \n",
       "10  The relationship between a person's inherent q...  \n",
       "11  The question of whether a person's identity is...  \n",
       "12  The notion of \"perfection\" in AI is complex an...  \n",
       "13  The concept of a \"perfect AI\" is inherently co...  \n",
       "14  The concept of a \"perfect AI\" is largely theor...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to comparison_of_single_prompt_responses_8.csv\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    for model_name in models.keys():\n",
    "        response = generate_response(model_name, prompt)\n",
    "        results.append({\"Model\": model_name, \"Prompt\": prompt, \"Response\": response})\n",
    "\n",
    "# Convert results to df\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Show results\n",
    "from IPython.display import display\n",
    "display(df_results)\n",
    "\n",
    "# Export results\n",
    "df_results.to_csv(\"comparison_of_single_prompt_responses_8.csv\", index=False)\n",
    "print(\"Results saved to comparison_of_single_prompt_responses_8.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be989f-2486-4f35-88b8-132739dec569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
